{\rtf1\ansi\ansicpg1252\cocoartf1504\cocoasubrtf820
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
\paperw11900\paperh16840\margl1440\margr1440\vieww8060\viewh14580\viewkind0
\deftab720
\pard\pardeftab720\sl280\partightenfactor0

\f0\fs24 \cf2 \expnd0\expndtw0\kerning0
This chapter presents the verification in more details. The different methods that were used to checked the model are outlined. This is done by looking through the code and outlining the issues that arose for the different parts of the code.\
\uc0\u8232 %%\
\\section\{The belief hierarchy\}\
\
The belief hierarchy is one of the most complex part of the model. The belief hierarchy structure contains the beliefs of the agent plus the belief of all other agents (the partial knowledge). This is built into a multi-dimensional array.\uc0\u8232 \
The belief hierarchy is present throughout the model in most functions. The fact that it is such a complex array means that verification is required throughout to make sure that every time a part of this belief hierarchy is selected, the right indexes are chosen. If the right indexes are not chosen, the code will still run but the results will be completely wrong. This is particularly important for the causal relations which are saved in the array in a certain sequence mentioned within the code comments. Without following this sequence, the code would run but the results would be flawed as it would use the wrong causal relations.\
\uc0\u8232 %-\
\\subsection\{Preference calculation\}\
\
For the preference calculation, several checks were performed to make sure the right indexes are selected. The preference calculation is performed for the agent's own beliefs but also for all the partial belief hierarchies in the belief hierarchy parameter. This is needed for the calculation of the best actions later on in the model. For the principle belief calculation, the selection of the right issue was checked, and it was made sure that the preferences of the two issues added up to one. Note that this preference calculation works regardless of the hierarchy structure. Nothing has been hardcoded.\
\
For the policy core belief preference calculations, the selection of the right issue was checked. The selection of only the causal relations with matching sign to the gap in the principle beliefs are also checked. Finally, it was checked that all the preferences on that level add up to one.\uc0\u8232 \
For the secondary belief preference calculation, a check was performed that only the issues related to the issue on the agenda are selected. Then the same checks were performed as the checks performed for the policy core beliefs.\
\
For the preference calculation for the policy hierarchies, the same procedures were performed but with the different code considering each of the impacts for each of the instruments. For all these calculations, checks were performed throughout the coding of the rest of the code. Through these checks, it was uncovered that the wrong indexes were chosen in some parts of the preferences calculation. It can now be said with certainty that the right indices are being selected and that the preference calculations are correct.\
\uc0\u8232 %-\
\\subsection\{Issue/instrument/policy/problem selection\}\
\
The instrument selection was checked by comparing the actual instrument selected and the preferences of all instruments. The instrument with the highest preference must be the one that is selected by the agent. The same was done for the policies and problems for the three streams model. Check were also performed to make sure that the grade list matches the length of the number of issues being considered.\
\uc0\u8232 %%\
\\section\{The individual actions selection and grading\}\
\
Before any actions are performed, the resources are provided to the agents. This was checked through print functions to make sure that the resources were different for agents with varying affiliations as they should be and followed the representation of the affiliation. Furthermore, the resources are then split, for the policy makers and policy entrepreneurs. 20\\% goes to the policy network actions while 80\\% go to the individual agent actions. This was checked to make sure that the resources are divided properly.\
\
%-\
\\subsection\{Network upgrade and maintenance actions\}\
\
The network actions are then performed by all active agents.  For this, it was made sure that the list of agents is shuffled so that it is always a different agent that is selected first. Two algorithms are used here. One for the agenda setting and one for the policy formulation. For each two strategies are possible as developed in the formalisation.\
\
%\
\\paragraph\{Agenda setting\}\
\
For the first strategy, checks were performed on the while loops. These loops allow actions to be performed as long as enough resources are left. Checks were then performed to see whether the links added to the list of links to be maintained did indeed meet the requirements set by this strategy (lower than 30\\% awareness but above 0). It was also checked that the list of links and its associated list of awareness values were coherent with respect to indexes. Finally, checks were also performed to make sure that all links related to the agent performing the action are selected and not all links within the model.\
\
Then for the actual maintenance of the links, it was checked that the right index of the link with the lowest awareness was selected in the list previously established. Then it was checked that the maintenance was duly performed. It was also checked that the affiliation be appropriately taken into account when increasing the awareness level. It was also made sure that if the list of links to maintain is empty, then the code moves to the next possible link maintenance. It was also checked that after each maintenance of a link, the appropriate resources are removed from the agent\'92s resources associated to link maintenance.\
\
For the creation of new links, it was made sure that this is only performed when all active links from the agent concerned are above 30\\%. Then a check was performed to see that only links with zero awareness within the agent\'92s network be selected for the creation of a new link (new links can only be created between agents that know that they exist hence not selecting awareness -1 links). Similarly to before, it was checked that when creating a link, the appropriate awareness be bestowed upon the new link and the resources be removed from the agent\'92s available maintenance resources. \\emph\{Looking at the code after implementation, it was found that the wrong equation was used for the creation of new links with the omission of the 0.5 coefficient. This had been added to the Further Work list in \\autoref\{cha:furtherWork\}.\} \
\
Finally, for the third step, it was checked that this step be performed only if resources are left through the same while loop as before. It was checked that only links that are below 1 and not equal to -1 in awareness be considered. Similarly to before, checks were performed to make sure that the links be added the right amount of resources depending on the affiliations and the resources. it was also made sure that the resources be removed from the agent\'92s resources after the action is performed. Finally, it was checked that if a link is maintained to a level higher than 1, its awareness be reduced back to 1 (no link is allowed to grow beyond 1 awareness).\
\
For the second strategy, similar checks were performed. The main difference here is the order in which the steps are made and which steps are used. For example, it was checked that the agents must have similar beliefs. For this, it was checked and verified that the agents\'92 aim of the problem be within 0.2 of one another for the three streams theory and the agents\'92 aim of the issue be within 0.2 of one another for all other models. It was checked that the list of links considered is then appropriately formed. Then overall the checks are the same.\
\
%\
\\paragraph\{Policy formulation\}\
\
For the policy formulation, the checks are similar to the checks of the agenda setting process and the strategies are identical. The main difference arose in the second strategy and the definition of similar beliefs. Checks were performed to make sure that the similar beliefs relate to the issue or problem on the agenda in each cases. The rest of the code that was used was the same as the one for the agenda setting process.\
\
%-\
\\subsection\{External parties actions [Backbone/Backbone+/ACF]\}\
\
The external parties can perform two actions and these actions are different in the agenda setting process and the policy formulation process. They are also different in the three streams theory as there a policy and a problem can be present. The resources for the external parties are split in two: 50\\% for the blanket framing and 50\\% for the electorate influence. The verifications performed are shown here.\
\
%\
\\paragraph\{Blanket framing (AS)\}\
\
The first checks performed for the blanket framing relate to the causal relations. Not all causal relations can be used for framing but only the ones related to the issue selected by the external party, It was therefore checked that the right causal relations are being selected. Then it was checked that the while loop used to make sure that the agent has enough resources does indeed work. Then for the grading of the actions, it was checked that the actions are graded appropriately based on the equations in the formalisation. It was also made sure that in case there is no partial knowledge, the partial knowledge be set to 0 so that calculations can be performed. It was checked that this be temporary and the None partial knowledge be re-applied after the action has been graded.\
\
For the assessment of the list of grades, it was checked that the right action is selected by checking the grades through a print command. Then it was checked that the action is appropriately applied to the right agent. For this several checks were performed by changing the number of agents manually and the number of causal relations. It was extensively cross checked with the number of grade recorded on in the lists of actions. Note that through this check, it was found several times that the actions performed were the wrong influence on the wrong agents. This has now been fixed. Finally, it was also checked that the resources be removed properly from the agent\'92s available resources for blanket framing actions.\
\
%\
\\paragraph\{Electorate influence (AS)\}\
\
For the electorate influence, the actions are performed differently. This is because the new preference of the electorates is calculated used to obtain the grade. This required the copy of some of the data to have temporary changes in the beliefs of the electorates. This was extensively checked as it is known that copy functions can lead to issue with the reference to memory. It was therefore made sure that copying the data did not have an impact on the rest of the simulation later on. Associated with this action, the preference calculation of the electorate was also verified. This was done similarly to how the verification of the preference calculations of the other agent is performed. This is because the code is mostly the same, simply adjusted for the electorate. It was also checked that the grades assigned are the appropriate ones and that they are stored in the right order.\
\
Finally, similarly to the blanket framing, the implementation of the actions was checked several times. This was once again done to make sure that the right action is applied on the right electorate. Furthermore, it was checked that the right amount of resources are removed from the agent\'92s resources after each implementation of an action.\
\
%\
\\paragraph\{Blanket framing (PF)\}\
\
The main difference between the agenda setting and the policy formulation is the choice of causal relations. This was therefore implemented and checked to make sure that the actions are now performed on the causal relations related to the agenda chosen by the actors. Considering most of the code was re-used from the previous agenda setting section, the checks performed were then the same.\
\
%\
\\paragraph\{Electorate influence (PF)\}\
\
This is similar for the electorate influence. The only difference between agenda setting and policy formulation is related to the issues that are being influenced. The other checks were the same as the ones presented before.\
\
%-\
\\subsection\{External parties actions [3S]\}\
\
For the three streams theory, most of the code was changed. This is because the agents are not using issues anymore but they are using a policy or a problem. Therefore, although the code infrastructure remains the same, most of the code had to be rewritten. Checks were used to make sure that the agents are performing the actions based on their initial choices between problem and policy. The same checks where then performed as previously. For the problem, the choice of causal relations were checked while for the policies, the choice of impact of the policies was checked. The equations to calculate the grade of each of the actions and the implementation of the actions were also checked. This is valid for both the agenda setting process and the policy formulation process.\
\
%-\
\\subsection\{Policy makers and entrepreneurs actions [Backbone/Backbone+/ACF]\}\
\
The actions of the policy entrepreneurs and policy makers are exactly the same. They are constructed in a similar fashion to the actions of the external parties. The main differences are the types of actions that are available to them. This is outlined here.\
\
%\
\\paragraph\{Agenda setting\}\
\
There are three type of actions that the actors can perform: framing, state influence and aim influence. All of these actions are assessed at the same time and the grades are compiled into a long list. This is done for each agent. Once the list has been complied, the action with the highest grade is selected and it is implemented. The list length is therefore the number of causal relations plus two times the amount of agents that are connected to the agent performing the actions and which have an awareness higher than 0. The first verification is performed on the creation of this grade list. It is made sure that only the appropriate are considered for actions. Then it is checked that all actions grades are obtained appropriately. The checks are mostly important for the temporary assignment of the value 0 when no partial knowledge is present in the agent\'92s belief hierarchy. This has shown to cause memory assignment issues in the past.\
\
Then comes the part where the best action is selected. It easy to select the action by simply finding the minimum grade. It is however trickier to define what action that is and on which agent. This was therefore verified after it was found that the wrong actions were selected. \
\
Then there is implementation of the actions. Depending on the action selected the action is implemented on specific actors. The equations here are mostly the same as the ones used for the assessment of the grade. The main different is the use of actual belief and not the partial beliefs anymore. The actions were thoroughly checked to make sure the right outcome is produced. A check in the code is also added to make sure that no beliefs goes above one or below minus one.\
\
%\
\\paragraph\{Policy formulation\}\
\
For the policy formulation, the steps are broadly the same. The main difference here relates in which issues are chosen and which causal relations are chosen. Once that is verified, the rest of the code is broadly the same and the verification checks used are also the same.\
\
%-\
\\subsection\{Policy makers and entrepreneurs actions [3S]\}\
\
Similarly to the addition for the external parties, the addition from the three streams model to the actions of the policy makers and policy entrepreneurs are significant. They required the writing of an entire new code but based using the infrastructure of the other models. The actions related to the problem are mostly the same as the one for the other models. The verification procedure is therefore the same. The main issue there was to identify the right indexes in the belief hierarchies of the actors as the notation is different between problem and issue.\
\
For the policy, an entirely new code has to be put in place. It provides the same state and aim influence actions but a completely new impact framing action that had to be verified to make sure that the impact is calculated appropriately.\
\
Beyond these changes, the rest of the code is very similar in architecture. The grades are placed in a list (different for the problem and the policy) and the lowest grade is the one selected to be implemented. Then it becomes a question of finding out what exactly that action was and to implement it on the right actor.\
\
Checks were performed throughout the code (and the infrastructure is still there). This was done through print functions and in some case where the grade list was complex, by manually checking that the right grade is being applied. \
\
%%\
\\section\{The team algorithms\}\
\
The creation of the team follows a complex algorithms that is outlined in the formalisation. This part of the code was the most challenging one as it dives deep into the object oriented part of the implementation mixed with the lists in which most of the objects are being stored. Groups are formed both in the agenda setting and policy formulation processes. Agents can only be in one group in each of the processes.\
\
%-\
\\subsection\{Agenda setting\}\
\
The team algorithm is a long process of steps that the agent has to go through to see if he can join a team, create a team or leave a team. \
\
The first step is to check if the agent has a team and if so to calculate its belonging level. This belonging level is calculated in a specific function that is used throughout the code. This function was checked to make sure that the belonging level is calculated appropriately. This was done by first checking that the same issue is selected by all agents considered. Then the average belief calculated was checked to make sure it adds up. Finally, it was checked that the belonging level calculated from this average is appropriately placed within the agent\'92s attributes.\
\
The second step simple checks whether an agent has enough belonging level to remain in the team or not. If that agent is the leader then the team would have to be disbanded. This was checked by assigning belonging levels lower than 30\\% to agents to see whether the code worked.\
\
The third step consists of checking whether the agent meets the requirement to be part of the team (if s/he is in a team). Two cases must be distinguished there with the agent being checked being the leader or just being a member. If the agent is the leader and s/he does not belong in the team anymore, the team must be disbanded. If s/he is just a member, then s/he only needs to be removed from the team member list. Throughout the verification of this step, issues arose. The problem was found to be related to the way an agent is removed from the member list. This lead to memory assignment issues within the list members and the code would crash. This has now been fixed and the members are appropriately removed. When a team is disbanded, it is not deleted, it is just removed from the attributes of the agents that were in that team. The main reason to keep the team is for records keeping. This was checked carefully to make sure that the data can be saved when it is collected.\
\
The check of the beliefs is done along two lines depending on whether the team is a problem team or a policy team. The verification here focused on checking that the appropriate equations are being used and the appropriate indexes in the belief hierarchies are selected. In some instances, it was found that the indexes were and this has since been corrected.\
\
After removal of an agent, then the belonging level has to be recalculated. This was checked to make sure that the belonging level of all agents present in the team are upgraded according to the new level.\
\
The fourth step is to check, if the agent is not in a team, whether the agent can join a team that already exists. The verification here is mostly the same as previously as the requirements are the same. The verification was focused on making sure that the right issues are selected depending on whether the agent is looking at a policy or problem team. And the indexes used were also checked to make sure the right issue is selected.\
\
Finally, the fifth and last step is the creation of a team if the agent still has no team. Again, the requirements here are similar to the ones previously outlined and so is the verification. Additional steps were taken to verify that the resources used are appropriately removed from the agent\'92s attributes. It was also made sure that the appropriate beliefs are used for the creation of the teams as for the first step, partial knowledge is used while for the actual creation check the full beliefs are used. Finally, and this is a big part of the creation of the team, it was checked that overtime there is a contact between agents they provided one another with their beliefs. This was checked and for each of the interactions, there was a check to make sure that the partial knowledge cannot be above one or below minus one.\
\
Checks were also performed on the creation of the teams themselves. It was made sure that the teams are added to the overall list of teams. It was checked that each of the agents considered were added to the list of members in the team. It was made sure that all agents that are part of the team have their attributes updated accordingly and their belonging level checked.\
\
Upon the creation of a team, a shadow network is created. This is in effect the policy network of the team which is created from the network of the team\'92s members. This shadow network created a number of problem as it required the creation of an entirely new network several times leading to a large amount of links. Each of these networks were then stored into arrays associated with the team. This shadow network creation was checked to make sure the right amount of links were added and that they were provided with the correct awareness levels.\
\
Note that these checks were performed for both strategies that are used to create new teams. The checks were fairly similar as the code infrastructure was the same.\
\
%-\
\\subsection\{Policy formulation\}\
\
For the policy formulation, the architecture of the code is mostly the same. The verification steps were therefore similar. The main difference as mentioned previously is the change of issues being considered. This was checked thoroughly to make sure the right issues are addressed at this level of the model.\
\
%%\
\\section\{The coalition algorithms\}\
\
The coalitions are created following what is outlined in the formalisation.  The first problem here is to make sure that the right future coalition leader is being chosen. This is particularly important when one coalition has already been created. The agents must not already be in another coalition so the total amount of awareness needs to be recalculated. This was checked to make sure that no agent is found in more than one coalition at a time.\
\
Then the issue is to check that that the right agents are considered to be inserted in a coalition. This is defined based on the beliefs and based on the policy networks of the lead agent. This is again a question of checking the indexes in such a way that the proper issues are considered by the team leader.\
\
The main difference between the agenda setting and the policy formulation processes is that the issue around which the coalitions are created are different. It was therefore important to check that the right issue is being considered in both cases.\
\
Checks were also performed on the fact that the coalition must be placed in the coalition list so that it can be recorded. It was also important to check that the agents attributes are appropriately changed when they join a coalition. Finally, it was important to check that the creation of coalition stop at the right amount (in this case less than 10\\% of the actors are coalition-less).\
\
%%\
\\section\{The teams actions selection and grading\}\
\
The actions of the teams are split in two parts: the intra-team actions and the inter-team actions. As mentioned previously in the formalisation, the former are about framing actions within the teams while the latter about actions from the teams on outside agents. These are therefore two very different parts of the code.\
\
The actions were mostly verified in the same way as the actions of the agents. The main difference here was for the inter-team actions which were performed by all actors within the team onto all actors that are within the policy network of the team. This sometimes resulted in hundreds of actions being assessed. It was therefore paramount to rightly pinpoint the right actions, who performed it, onto who and about which issue. This was checked through a multitude print function which are still present in the code. Furthermore, a big problem here was the notation system of Python that considers that the first entry in a list is numbered 0. This leads to multiple attempts were the wrong index was selected. Ultimately, this was fixed and it is now provided with certainty that the right actions are performed.\
\
%%\
\\section\{The coalitions actions selection and grading\}\
\
Similarly to the team actions, the coalition actions are modelled on the individual agent actions. They were therefore verified in the same way. The coalition action are much simpler as they are performed by the coalition leader. This reduced the number of actions considered drastically and made it easier to pinpoint which actions should be implemented.\
\uc0\u8232 %%\
\\section\{The awareness decay\}\
\
The awareness decay is applied at the end of the tick. This was checked by changing the value of decay to this if it works properly. Furthermore, it was checked that the awareness decay pause that is established after an action has been performed worked appropriately. This was done by changing the amount of time after which the awareness decay is paused.\
\
%%\
%\\section\{The technical model\}\
\
%%\
\\section\{The initialisation\}\
\
For the initialisation, all of the inputs that are specified by the modeller are placed into a dictionary. This is then transmitted through the function and classes. To verify this dictionary, each of its entries are checked in the main class from which the model is run and all of its contents are re-assigned to the actual parameters from the model. This dictionary was used to simplify the transmission of the inputs from the initialisation file to the main file. Note that this approach can be used for any future case study.\
\
The initialisation is also a large file that constitutes the first list of agents present in the model and the policy network. This was all checked by using print function to make sure that the right amount of resources are added or that the right links are created. Furthermore, checks are in place to make sure that the initialised beliefs of all the agents are below one and above minus one.\
\
%%\
\\section\{The data collection\}\
\
The data collection is a complex process that uses the architecture of the code used by Project Mesa. The original code used deep copy everywhere to appropriately copy the data into new data framed. However, this takes a very large amount of times within the model implemented (upwards of 4 hours per tick for larger models). It was therefore to change the deep copy approach to a simpler approach using copy.copy. This lead to different problems such as memory assignment issues. Ultimately, it was settled to have a mix of deep copy and copy.copy throughout the code. This was intensely verified to make sure that there are no more memory assignment issues.\
\
%%\
%\\section\{The transfer of the states to the agents\}\
\
%The transfer of the states to the agent is different\
\
%States update of the electorate\
\
%State update of the policy entrepreneurs\
\
%States update of the truth agent\
\
%States update of the policy makers\
\
%States update of the external parties}